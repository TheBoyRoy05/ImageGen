{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36970470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import logging\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from modules import UNet, Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2efecb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9f4677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    setup_logging(args.run_name)\n",
    "    dataloader = get_data(args)\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    model = UNet(device=args.device).to(args.device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=args.device)\n",
    "    \n",
    "    logger = SummaryWriter(os.path.join(\"runs\", args.run_name))\n",
    "    l = len(dataloader)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        pbar = tqdm(dataloader)\n",
    "        for i, (images, _) in enumerate(pbar):\n",
    "            images = images.to(args.device)\n",
    "            t = diffusion.sample_timesteps(images.shape[0]).to(args.device)\n",
    "            x_t, noise = diffusion.noise_images(images, t)\n",
    "            predicted_noise = model(x_t, t)\n",
    "            loss = mse(noise, predicted_noise)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(MSE=loss.item())\n",
    "            logger.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(\"models\", args.run_name, f\"ckpt.pt\"))\n",
    "        sampled_images = diffusion.sample(model, n=images.shape[0])\n",
    "        save_images(sampled_images, os.path.join(\"results\", args.run_name, f\"{epoch}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b8a01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.run_name = \"DDPM_Uncondtional\"\n",
    "    args.epochs = 500\n",
    "    args.batch_size = 12\n",
    "    args.image_size = 64\n",
    "    args.dataset_path = r\"C:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\landscape_imgs\"\n",
    "    args.device = \"cpu\"\n",
    "    args.lr = 3e-4\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd027604",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mlaunch\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m args.device = \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m args.lr = \u001b[32m3e-4\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(args):\n\u001b[32m      2\u001b[39m     setup_logging(args.run_name)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     dataloader = \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     mse = nn.MSELoss()\n\u001b[32m      6\u001b[39m     model = UNet(device=args.device).to(args.device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\utils.py:31\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_data\u001b[39m(args):\n\u001b[32m     25\u001b[39m     transforms = torchvision.transforms.Compose([\n\u001b[32m     26\u001b[39m         torchvision.transforms.Resize(\u001b[32m80\u001b[39m),  \u001b[38;5;66;03m# args.image_size + 1/4 *args.image_size\u001b[39;00m\n\u001b[32m     27\u001b[39m         torchvision.transforms.RandomResizedCrop(args.image_size, scale=(\u001b[32m0.8\u001b[39m, \u001b[32m1.0\u001b[39m)),\n\u001b[32m     28\u001b[39m         torchvision.transforms.ToTensor(),\n\u001b[32m     29\u001b[39m         torchvision.transforms.Normalize((\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m), (\u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.5\u001b[39m))\n\u001b[32m     30\u001b[39m     ])\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     dataset = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[39m, in \u001b[36mImageFolder.__init__\u001b[39m\u001b[34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    327\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.imgs = \u001b[38;5;28mself\u001b[39m.samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:150\u001b[39m, in \u001b[36mDatasetFolder.__init__\u001b[39m\u001b[34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(root, transform=transform, target_transform=target_transform)\n\u001b[32m    149\u001b[39m classes, class_to_idx = \u001b[38;5;28mself\u001b[39m.find_classes(\u001b[38;5;28mself\u001b[39m.root)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m samples = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mself\u001b[39m.loader = loader\n\u001b[32m    159\u001b[39m \u001b[38;5;28mself\u001b[39m.extensions = extensions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:203\u001b[39m, in \u001b[36mDatasetFolder.make_dataset\u001b[39m\u001b[34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_to_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    199\u001b[39m     \u001b[38;5;66;03m# prevent potential bug since make_dataset() would use the class_to_idx logic of the\u001b[39;00m\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# find_classes() function, instead of using that of the find_classes() method, which\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# is potentially overridden and thus could have a different logic.\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe class_to_idx parameter cannot be None.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmake_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\issac\\OneDrive\\Projects\\ImageGen\\.venv\\Lib\\site-packages\\torchvision\\datasets\\folder.py:89\u001b[39m, in \u001b[36mmake_dataset\u001b[39m\u001b[34m(directory, class_to_idx, extensions, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(target_dir):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m root, _, fnames \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os.walk(target_dir, followlinks=\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(fnames):\n\u001b[32m     91\u001b[39m         path = os.path.join(root, fname)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:368\u001b[39m, in \u001b[36m_walk\u001b[39m\u001b[34m(top, topdown, onerror, followlinks)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
